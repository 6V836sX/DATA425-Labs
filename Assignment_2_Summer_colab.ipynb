{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/6V836sX/DATA425-Labs/blob/main/Assignment_2_Summer_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ZYRAQcq-uPLL",
      "metadata": {
        "id": "ZYRAQcq-uPLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c907f5-cbd0-4ddc-9c68-24d980a298d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# æŒ‚è½½ Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import os, shutil, pathlib\n",
        "\n",
        "# src = \"/content/drive/My Drive/Colab Notebooks/DATA425A2/data/CUB_200_2011/images\"\n",
        "# dst = \"/content/images\"\n",
        "# if not pathlib.Path(dst).exists():  # é¿å…é‡å¤å¤åˆ¶\n",
        "#     print(\"Copying images to local VM...\")\n",
        "#     shutil.copytree(src, dst)\n",
        "# else:\n",
        "#     print(\"Images already copied.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9lYqaoQprdm4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lYqaoQprdm4",
        "outputId": "5be62d7d-b1b2-4432-d11e-4b0311e40ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ æ•°æ®é›†è·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»ºä¿å­˜ç›®å½•...\n",
            "âœ… Dataset path å·²å­˜åœ¨: /content/drive/MyDrive/Colab Notebooks/DATA425A2/data/CUB_200_2011/images\n",
            "ğŸ“ Checkpoint dir å·²å­˜åœ¨: /content/drive/MyDrive/Colab Notebooks/DATA425A2/checkpoints\n",
            "ğŸ“ Log dir å·²å­˜åœ¨: /content/drive/MyDrive/Colab Notebooks/DATA425A2/logs\n",
            "ğŸ“ Result dir å·²å­˜åœ¨: /content/drive/MyDrive/Colab Notebooks/DATA425A2/results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# è®¾ç½®åŸºç¡€è·¯å¾„\n",
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/DATA425A2'\n",
        "\n",
        "# å­è·¯å¾„\n",
        "# data_dir = '/content/images'\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/DATA425A2/data/CUB_200_2011/images'\n",
        "base_ckpt_dir = os.path.join(base_dir, 'checkpoints')\n",
        "base_log_dir = os.path.join(base_dir, 'logs')\n",
        "base_result_dir = os.path.join(base_dir, 'results')\n",
        "notebook_dir = os.path.join(base_dir, 'notebook')\n",
        "\n",
        "# === è·¯å¾„æ£€æŸ¥ ===\n",
        "required_paths = {\n",
        "    \"âœ… Dataset path\": data_dir,\n",
        "    \"ğŸ“ Checkpoint dir\": base_ckpt_dir,\n",
        "    \"ğŸ“ Log dir\": base_log_dir,\n",
        "    \"ğŸ“ Result dir\": base_result_dir,\n",
        "}\n",
        "\n",
        "# æ£€æŸ¥ data_dir æ˜¯å¦å­˜åœ¨ï¼ˆå¿…é¡»ï¼‰\n",
        "if not os.path.exists(data_dir):\n",
        "    raise FileNotFoundError(f\"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨ï¼š{data_dir}\\nè¯·æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è‡³ Google Drive å¹¶å‘½åæ­£ç¡®ã€‚\")\n",
        "\n",
        "print(\"ğŸ‰ æ•°æ®é›†è·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»ºä¿å­˜ç›®å½•...\")\n",
        "\n",
        "# å…¶ä½™è·¯å¾„ï¼šå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º\n",
        "for desc, path in required_paths.items():\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        print(f\"{desc} åˆ›å»ºæˆåŠŸ: {path}\")\n",
        "    else:\n",
        "        print(f\"{desc} å·²å­˜åœ¨: {path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33dff607-2f32-4904-b061-5b552fb2d430",
      "metadata": {
        "id": "33dff607-2f32-4904-b061-5b552fb2d430"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "import os\n",
        "def plot_loss_accuracy(history):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss å’Œ sparse_categorical_accuracy æ›²çº¿ã€‚\n",
        "    å‚æ•°ï¼š\n",
        "        history : tf.keras.callbacks.History å¯¹è±¡\n",
        "    \"\"\"\n",
        "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    historydf[[\"loss\", \"val_loss\", \"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\"]].plot(\n",
        "        ylim=(0, max(1.0, historydf.values.max())),\n",
        "        title=\"Training and Validation Loss / Accuracy\",\n",
        "        grid=True,\n",
        "        figsize=(10, 6)\n",
        "    )\n",
        "\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_acc = history.history['sparse_categorical_accuracy'][-1]\n",
        "    plt.title(f'Final Loss: {final_loss:.3f}, Final Accuracy: {final_acc:.3f}')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_multiple_histories_with_annotations(histories, labels=None, metric=\"sparse_categorical_accuracy\",\n",
        "                                             figsize=(14, 6), save_path=None, dpi=300, file_format=\"png\"):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶å¤šä¸ª history å¯¹è±¡çš„è®­ç»ƒ/éªŒè¯æ›²çº¿ï¼ˆæ”¯æŒä¸åŒé¢œè‰²/çº¿å‹ï¼Œæ ‡æ³¨æœ€é«˜ç‚¹ï¼Œå¯¼å‡ºå›¾åƒï¼‰\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "    - histories: list of tf.keras.callbacks.History objects\n",
        "    - labels: list of str, ç”¨äºæ ‡æ³¨æ¯ä¸ªæ¨¡å‹\n",
        "    - metric: str, è®­ç»ƒæŒ‡æ ‡åï¼Œå¦‚ \"sparse_categorical_accuracy\", \"loss\"\n",
        "    - figsize: tuple, å›¾åƒå°ºå¯¸\n",
        "    - save_path: str, æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆæ— æ‰©å±•åï¼‰\n",
        "    - dpi: int, å¯¼å‡ºå›¾åƒåˆ†è¾¨ç‡\n",
        "    - file_format: str, 'png' æˆ– 'svg'\n",
        "    \"\"\"\n",
        "\n",
        "    if labels is None:\n",
        "        labels = [f\"Model {i+1}\" for i in range(len(histories))]\n",
        "\n",
        "    colors = plt.cm.get_cmap('tab10', len(histories))  # ä¸åŒæ¨¡å‹ä¸åŒé¢œè‰²\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # === å­å›¾1ï¼šè®­ç»ƒæ›²çº¿ï¼ˆå®çº¿ï¼‰\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
        "        plt.plot(hist.epoch, hist.history[metric], linestyle='-', color=colors(i), label=f\"{label} (train)\")\n",
        "    plt.title(f\"Training {metric}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # === å­å›¾2ï¼šéªŒè¯æ›²çº¿ï¼ˆè™šçº¿ + æ ‡æ³¨ï¼‰\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, (hist, label) in enumerate(zip(histories, labels)):\n",
        "        val_metric = f\"val_{metric}\"\n",
        "        val_values = hist.history[val_metric]\n",
        "        epochs = hist.epoch\n",
        "        plt.plot(epochs, val_values, linestyle='--', color=colors(i), label=f\"{label} (val)\")\n",
        "\n",
        "        # è‡ªåŠ¨æ ‡æ³¨æœ€å¤§ val accuracy ä½ç½®\n",
        "        best_epoch = int(pd.Series(val_values).idxmax())\n",
        "        best_value = val_values[best_epoch]\n",
        "        plt.scatter(best_epoch, best_value, color=colors(i), marker='o')\n",
        "        plt.text(best_epoch, best_value + 0.01, f\"{best_value:.3f}\", fontsize=9, ha='center', color=colors(i))\n",
        "\n",
        "    plt.title(f\"Validation {metric}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # å¯¼å‡ºå›¾åƒ\n",
        "    if save_path:\n",
        "        full_path = f\"{save_path}.{file_format}\"\n",
        "        plt.savefig(full_path, dpi=dpi, format=file_format)\n",
        "        print(f\"âœ… å›¾åƒå·²ä¿å­˜ä¸º {full_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adf62a5cc3f776da",
      "metadata": {
        "id": "adf62a5cc3f776da"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d60eaf0e-4f0e-48a6-babf-cea234893c53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60eaf0e-4f0e-48a6-babf-cea234893c53",
        "outputId": "7a4deb9d-160a-4e27-c452-babc5d80f248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11788 files belonging to 200 classes.\n"
          ]
        }
      ],
      "source": [
        "# ImageNet normalization stats\n",
        "IMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\n",
        "IMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n",
        "\n",
        "# Custom preprocessing function\n",
        "def preprocess_train(image, label):\n",
        "    image = tf.image.resize_with_pad(image, 256, 256)  # çŸ­è¾¹ç¼©æ”¾åˆ°256ï¼Œpadé•¿è¾¹\n",
        "    image = tf.image.random_crop(image, size=(224, 224, 3))  # éšæœºè£å‰ª\n",
        "    image = tf.image.random_flip_left_right(image)  # éšæœºæ°´å¹³ç¿»è½¬\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)  # æ˜äº®åº¦æŠ–åŠ¨\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # å½’ä¸€åŒ–åˆ°0~1\n",
        "    image = (image - IMAGENET_MEAN) / IMAGENET_STD  # ä½¿ç”¨ImageNetå‡å€¼æ ‡å‡†åŒ–\n",
        "    return image, label\n",
        "\n",
        "def preprocess_val(image, label):\n",
        "    image = tf.image.resize_with_pad(image, 256, 256)\n",
        "    image = tf.image.central_crop(image, central_fraction=0.875)  # è¿‘ä¼¼224 crop\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    return image, label\n",
        "\n",
        "# Load raw dataset\n",
        "raw_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/DATA425A2/data/CUB_200_2011/images',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(256, 256),  # åˆæ­¥resizeåˆ°ç»Ÿä¸€å°ºå¯¸ï¼ˆä¸ä½œå˜å½¢ï¼‰\n",
        "    batch_size=None,  # è¿”å›æœªæ‰¹é‡åŒ–çš„ (image, label), ä¿è¯ map æ—¶ä¼ å…¥çš„æ˜¯å•å¼ å›¾åƒ\n",
        "    shuffle=True,\n",
        "    seed=888\n",
        ")\n",
        "\n",
        "# Train/Val split\n",
        "total_size = 11788  # CUB-200-2011 æ€»æ ·æœ¬æ•°\n",
        "train_size = 5994   # æŒ‰ç…§å®˜æ–¹ split\n",
        "val_size = total_size - train_size\n",
        "\n",
        "train_ds = raw_dataset.take(train_size).map(preprocess_train).batch(256).shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = raw_dataset.skip(train_size).map(preprocess_val).batch(256).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9978d60228b3a70",
      "metadata": {
        "id": "f9978d60228b3a70",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Check the dataset\n",
        "# for images, labels in train_ds.take(1):\n",
        "#     print(images.shape)  # (256, 224, 224, 3)\n",
        "#     print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35452960bfe97464",
      "metadata": {
        "id": "35452960bfe97464"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b50dd41-58cd-49dd-a1a2-f553771b21e7",
      "metadata": {
        "id": "6b50dd41-58cd-49dd-a1a2-f553771b21e7"
      },
      "source": [
        "## Model Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "57d3865f902d8c8",
      "metadata": {
        "id": "57d3865f902d8c8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet101V2\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "def build_model():\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    base_model = ResNet101V2(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
        "    base_model.trainable = True  # Full fine-tuning\n",
        "\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    # x = layers.Dropout(0.2)(x)  # optional\n",
        "\n",
        "    outputs = layers.Dense(200, activation=\"softmax\", name=\"Predictions\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    # model.summary(show_trainable=True)\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d5911e-1987-456b-956d-ccc398987796",
      "metadata": {
        "id": "c3d5911e-1987-456b-956d-ccc398987796"
      },
      "source": [
        "## Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb02032b6d9a0334",
      "metadata": {
        "id": "eb02032b6d9a0334"
      },
      "outputs": [],
      "source": [
        "def compile_model(model, lr=0.01, m=0.9, wd=0.0001):\n",
        "    optimizer = tf.keras.optimizers.SGD(\n",
        "        learning_rate=lr,\n",
        "        momentum=m,\n",
        "        weight_decay=wd  # TF â‰¥ 2.9 iff available, our tf.__version__ = 2.16\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce267ef04a30ccd8",
      "metadata": {
        "id": "ce267ef04a30ccd8"
      },
      "source": [
        "## Model Fitting Control"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d3e08c-2956-45d2-be9b-0d9a72437d8a",
      "metadata": {
        "id": "c9d3e08c-2956-45d2-be9b-0d9a72437d8a"
      },
      "source": [
        "### step decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cbbce7c5-8281-450c-b3e3-4e11a395b186",
      "metadata": {
        "id": "cbbce7c5-8281-450c-b3e3-4e11a395b186",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Step Decay LearningRateScheduler Functionï¼ˆLi et al., 2020ï¼‰\n",
        "def step_decay(epoch):\n",
        "    if epoch < 150:\n",
        "        return 0.01\n",
        "    elif epoch < 250:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c17076bb-3eb8-411c-ad74-96c4ce6fa650",
      "metadata": {
        "id": "c17076bb-3eb8-411c-ad74-96c4ce6fa650"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "eaa4580a-2296-4024-b751-21b4167ace79",
      "metadata": {
        "id": "eaa4580a-2296-4024-b751-21b4167ace79"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "\n",
        "checkpoint_path = \"checkpoints/best_model.keras\"\n",
        "log_dir = \"logs/single_run\"\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        mode=\"min\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir,\n",
        "        histogram_freq=1,\n",
        "        write_graph=True,\n",
        "        write_images=True\n",
        "    ),\n",
        "    lr_callback\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da92957e-430e-4681-acc3-e0447929a568",
      "metadata": {
        "id": "da92957e-430e-4681-acc3-e0447929a568"
      },
      "source": [
        "## Output Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9aa28934-fb4b-4cb8-ad11-ba9d1547eb76",
      "metadata": {
        "id": "9aa28934-fb4b-4cb8-ad11-ba9d1547eb76"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# output dir\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "base_ckpt_dir = f\"checkpoints_grid_{timestamp}\"\n",
        "base_log_dir = f\"logs_grid_{timestamp}\"\n",
        "os.makedirs(base_ckpt_dir, exist_ok=True)\n",
        "os.makedirs(base_log_dir, exist_ok=True)\n",
        "\n",
        "# result initial\n",
        "results = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b9131f-56c3-4ead-a093-513bd77b8564",
      "metadata": {
        "id": "d2b9131f-56c3-4ead-a093-513bd77b8564"
      },
      "source": [
        "# Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2bd0e428-fb4c-4de6-9957-b7982e273389",
      "metadata": {
        "id": "2bd0e428-fb4c-4de6-9957-b7982e273389"
      },
      "outputs": [],
      "source": [
        "def run_experiment(lr, m, wd=0.0, tag=\"\", epochs=300, use_early_stopping=False,\n",
        "                   train_ds=None, val_ds=None):\n",
        "    model = build_model()\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=m, weight_decay=wd)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,  # ä½ çš„ early stopping æˆ–å…¶ä»– callbacks\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # è¿”å›éœ€è¦ä¿å­˜çš„ç»“æœ\n",
        "    return {\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": m,\n",
        "        \"final_val_acc\": history.history[\"val_accuracy\"][-1],\n",
        "        \"history\": history\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU52SYC97Bm-",
        "outputId": "2680e0eb-7835-4640-c1ed-1d1b18b745f3"
      },
      "id": "XU52SYC97Bm-",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5494f2c0-0ba0-439f-8f2e-5b9d2b712590",
      "metadata": {
        "id": "5494f2c0-0ba0-439f-8f2e-5b9d2b712590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d70461-830c-4e1a-caa1-9269f38de24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: lr=0.0001, m=0.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m171317808/171317808\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 5.49421, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 2: val_loss improved from 5.49421 to 4.95655, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 3: val_loss improved from 4.95655 to 4.39520, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 4: val_loss improved from 4.39520 to 3.89061, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 5: val_loss improved from 3.89061 to 3.40381, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 6: val_loss improved from 3.40381 to 2.90624, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 7: val_loss improved from 2.90624 to 2.50031, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 8: val_loss improved from 2.50031 to 2.18055, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 9: val_loss improved from 2.18055 to 1.96274, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 10: val_loss improved from 1.96274 to 1.79022, saving model to checkpoints/best_model.keras\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.0001, m=0.8\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.79022\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.79022\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.79022\n",
            "\n",
            "Epoch 4: val_loss improved from 1.79022 to 1.73532, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 5: val_loss improved from 1.73532 to 1.48412, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 6: val_loss improved from 1.48412 to 1.32629, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 7: val_loss improved from 1.32629 to 1.21572, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 8: val_loss improved from 1.21572 to 1.18428, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 9: val_loss improved from 1.18428 to 1.12554, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.12554\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training: lr=0.0001, m=0.9\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.12554\n",
            "\n",
            "Epoch 9: val_loss improved from 1.12554 to 1.12036, saving model to checkpoints/best_model.keras\n",
            "\n",
            "Epoch 10: val_loss improved from 1.12036 to 1.04903, saving model to checkpoints/best_model.keras\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.0001, m=0.95\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training: lr=0.0001, m=0.99\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training: lr=0.0005, m=0.0\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.0005, m=0.8\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Training: lr=0.0005, m=0.9\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.0005, m=0.95\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.0005, m=0.99\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.001, m=0.0\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.001, m=0.8\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.04903\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.001, m=0.9\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.04903\n",
            "\n",
            "Epoch 10: val_loss improved from 1.04903 to 1.02649, saving model to checkpoints/best_model.keras\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.001, m=0.95\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.02649\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training: lr=0.001, m=0.99\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.02649\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training: lr=0.005, m=0.0\n",
            "\n",
            "Epoch 1: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 3: val_loss did not improve from 1.02649\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.02649\n"
          ]
        }
      ],
      "source": [
        "# Grid Search Parameter\n",
        "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "momentums = [0.0, 0.8, 0.9, 0.95, 0.99]\n",
        "results = []\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d2c7cfe-0275-455b-bc48-d8a69feee57e",
      "metadata": {
        "id": "3d2c7cfe-0275-455b-bc48-d8a69feee57e"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(f\"grid_search_results_{timestamp}.csv\", index=False)\n",
        "\n",
        "# æ›²çº¿å›¾ 1ï¼šä¸åŒå­¦ä¹ ç‡ä¸‹ val accuracy æ›²çº¿ï¼ˆæŒ‰ momentum åˆ†ç»„ï¼‰\n",
        "plt.figure(figsize=(10, 6))\n",
        "for m in sorted(results_df['momentum'].unique()):\n",
        "    subset = results_df[results_df['momentum'] == m]\n",
        "    plt.plot(subset['learning_rate'], subset['best_val_accuracy'], marker='o', label=f'm={m}')\n",
        "\n",
        "plt.title(\"Validation Accuracy vs Learning Rate\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Best Validation Accuracy\")\n",
        "plt.xscale('log')  # å¯¹å­¦ä¹ ç‡ä½¿ç”¨ log åæ ‡æ›´ç›´è§‚\n",
        "plt.legend(title=\"Momentum\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "acc_curve_path = f\"val_accuracy_curve_{timestamp}.png\"\n",
        "plt.savefig(acc_curve_path, dpi=300)\n",
        "plt.show()\n",
        "print(f\"âœ… æ›²çº¿å›¾ï¼ˆAccuracyï¼‰å·²ä¿å­˜ä¸ºï¼š{acc_curve_path}\")\n",
        "\n",
        "\n",
        "# æ›²çº¿å›¾ 2ï¼šä¸åŒåŠ¨é‡ä¸‹ val accuracy æ›²çº¿ï¼ˆæŒ‰ learning_rate åˆ†ç»„ï¼‰\n",
        "plt.figure(figsize=(10, 6))\n",
        "for lr in sorted(results_df['learning_rate'].unique()):\n",
        "    subset = results_df[results_df['learning_rate'] == lr]\n",
        "    plt.plot(subset['momentum'], subset['best_val_accuracy'], marker='o', label=f'lr={lr}')\n",
        "\n",
        "plt.title(\"Validation Accuracy vs Momentum\")\n",
        "plt.xlabel(\"Momentum\")\n",
        "plt.ylabel(\"Best Validation Accuracy\")\n",
        "plt.legend(title=\"Learning Rate\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "momentum_curve_path = f\"val_accuracy_vs_momentum_curve_{timestamp}.png\"\n",
        "plt.savefig(momentum_curve_path, dpi=300)\n",
        "plt.show()\n",
        "print(f\"âœ… æ›²çº¿å›¾ï¼ˆvs Momentumï¼‰å·²ä¿å­˜ä¸ºï¼š{momentum_curve_path}\")\n",
        "\n",
        "# çƒ­åŠ›å›¾\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ä¿å­˜ CSV æ–‡ä»¶\n",
        "results_df = pd.DataFrame(results)\n",
        "csv_path = f\"grid_search_results_{timestamp}.csv\"\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "print(f\"\\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{csv_path}\")\n",
        "\n",
        "# === è¾“å‡º Heatmap å›¾åƒ ===\n",
        "\n",
        "# Pivot æˆçŸ©é˜µå½¢å¼\n",
        "acc_matrix = results_df.pivot(index=\"momentum\", columns=\"learning_rate\", values=\"best_val_accuracy\")\n",
        "loss_matrix = results_df.pivot(index=\"momentum\", columns=\"learning_rate\", values=\"best_val_loss\")\n",
        "\n",
        "# ç»˜åˆ¶ Accuracy çƒ­åŠ›å›¾\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(acc_matrix, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Validation Accuracy Heatmap\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Momentum\")\n",
        "plt.tight_layout()\n",
        "acc_fig_path = f\"val_accuracy_heatmap_{timestamp}.png\"\n",
        "plt.savefig(acc_fig_path, dpi=300)\n",
        "plt.show()\n",
        "print(f\"âœ… å‡†ç¡®ç‡å›¾åƒå·²ä¿å­˜ä¸ºï¼š{acc_fig_path}\")\n",
        "\n",
        "# ï¼ˆå¯é€‰ï¼‰ç»˜åˆ¶ Loss çƒ­åŠ›å›¾\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(loss_matrix, annot=True, fmt=\".3f\", cmap=\"Reds_r\")\n",
        "plt.title(\"Validation Loss Heatmap\")\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Momentum\")\n",
        "plt.tight_layout()\n",
        "loss_fig_path = f\"val_loss_heatmap_{timestamp}.png\"\n",
        "plt.savefig(loss_fig_path, dpi=300)\n",
        "plt.show()\n",
        "print(f\"âœ… æŸå¤±å›¾åƒå·²ä¿å­˜ä¸ºï¼š{loss_fig_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a33c412-cc4c-4cda-a224-a882c5ead7b5",
      "metadata": {
        "id": "3a33c412-cc4c-4cda-a224-a882c5ead7b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (TensorFlow M1)",
      "language": "python",
      "name": "tf_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}